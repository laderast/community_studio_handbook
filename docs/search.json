[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "2 Getting better and upskilling\nOne of the things that has prevented me from getting better at things is the fear of looking foolish. It’s a hard feeling to shake. I still get this feeling from time to time.\nWhat is the antidote? I think that the answer lies in finding and participating in a supportive community. A community where it is safe to ask questions and not feel foolish. A community full of supportive people like me, who want to help me and are invested in my growth.\nThat’s what I want to help build at the Hutch. A group where we all want to upskilll and learn new things that help us kick ass. Where it’s ok to not know everything, because we know something and want to share.\nI think a big part of this is to establish values as a community. I’d like to avoid the situation where someone asks “Why didn’t you use X?” which I’ve always found accusatory, unsupportive, and unhelpful, especially coming from the mouths of principal investigators.\nSo I hope you’ll considering joining me in participating in this new learning community.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#making-mistakes-is-ok",
    "href": "intro.html#making-mistakes-is-ok",
    "title": "1  Introduction",
    "section": "2.1 Making Mistakes is OK",
    "text": "2.1 Making Mistakes is OK\nI’ve learned that to get better, you have to be willing to make a lot of mistakes. This is especially true when you are learning to speak a new language.\nAgain, it’s OK not to know how to do things. We have to remember that we are all beginners at something and experts at other things. Let’s be respectful of that.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "learning_paths.html",
    "href": "learning_paths.html",
    "title": "2  Learning Paths",
    "section": "",
    "text": "2.1 The Purpose of This Chapter\nThe purpose of this chapter is to list skills that are necessary to be successful in reproducibly running workflows on a High Performance Computing Cluster.\nThis chapter exists to help you find out what you know and what you might want to know. Each skill highlights what you can accomplish once you’ve mastered it.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learning Paths</span>"
    ]
  },
  {
    "objectID": "learning_paths.html#my-own-learning-journey",
    "href": "learning_paths.html#my-own-learning-journey",
    "title": "2  Learning Paths",
    "section": "",
    "text": "Up your bash skills\nBash Scripting\nLearning about HPC concepts and allocations\nUp your lnowledge about filesystems and naming\nWDL and Docker for reproducibility\nScattering your data and bringing it back together",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learning Paths</span>"
    ]
  },
  {
    "objectID": "learning_paths.html#up-your-shell-skills",
    "href": "learning_paths.html#up-your-shell-skills",
    "title": "2  Learning Paths",
    "section": "2.3 Up Your Shell Skills",
    "text": "2.3 Up Your Shell Skills\nLet’s face it - almost none of us have formal training in using the Unix Shell. And yet there are some of us who seem like wizards at the shell - their commands and flle names seem to complete if by magic, and they can run jobs and sign-off without interrupting their jobs.\nLet’s throw back the curtain once and for all - this is all knowledge you can pick up and excel at.\nWhat you can do now: Run background processes while being signed out, navigate relative paths, use keyboard shortcuts such as tab completion and history.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learning Paths</span>"
    ]
  },
  {
    "objectID": "learning_paths.html#bash-scripting-to-automate-the-boring-things",
    "href": "learning_paths.html#bash-scripting-to-automate-the-boring-things",
    "title": "2  Learning Paths",
    "section": "2.4 Bash Scripting to Automate the Boring Things",
    "text": "2.4 Bash Scripting to Automate the Boring Things\nNow we have some skill navigating the shell and our file system. We can start thinking about automating our work. We can take a list of files and process them with the same commands, we can schedule processes to run at night, and we can start adapting or R scripts to run on the command line.\nIt all starts with the parametrization of our scripts and learning how to pipe things: taking the output of one command and using it as the input for another command. Once we learn that, we can start building larger pipelines to process files.\nWhat you can do now: Build simple pipelines gluing multiple software together. Process a file with a script using our pipeline. Install software to a system without root access.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learning Paths</span>"
    ]
  },
  {
    "objectID": "learning_paths.html#learn-hpc-concepts",
    "href": "learning_paths.html#learn-hpc-concepts",
    "title": "2  Learning Paths",
    "section": "2.6 Learn HPC Concepts",
    "text": "2.6 Learn HPC Concepts\nWe’ve done everything so far on a single computer - now we can graduate to the HPC cluster. We need to learn a bit about the architecture of the cluster and how to use it.\nWe need to learn about the different types of nodes, how we can use them, and how to access software and data within the shared filesystem.\nWhat you can do now: visualize the flow of requesting an allocation, query the head node to see how busy the cluster is, understand our place in a job queue and how we can speed up processing jobs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learning Paths</span>"
    ]
  },
  {
    "objectID": "getting_good_with_the_shell.html",
    "href": "getting_good_with_the_shell.html",
    "title": "3  Getting good with the Shell",
    "section": "",
    "text": "3.1 Resources\nThe two best resources I can think of are:\nI plan on expanding my book Bash for Bioinformatics with more recipes for those wanting to upskill to HPC and Workflow Description Language (WDL).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting good with the Shell</span>"
    ]
  },
  {
    "objectID": "getting_good_with_the_shell.html#resources",
    "href": "getting_good_with_the_shell.html#resources",
    "title": "3  Getting good with the Shell",
    "section": "",
    "text": "The Missing Semester of your CS Education\nData Science on the Command Line.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Getting good with the Shell</span>"
    ]
  },
  {
    "objectID": "wdl_for_you_and_me.html",
    "href": "wdl_for_you_and_me.html",
    "title": "5  WDL for you and me",
    "section": "",
    "text": "5.1 What is WDL?\nWorkflow Description Language (WDL) is a way to describe a processing pipeline",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>WDL for you and me</span>"
    ]
  },
  {
    "objectID": "wdl_for_you_and_me.html#how-is-it-different-from-workflows-like-snakemake",
    "href": "wdl_for_you_and_me.html#how-is-it-different-from-workflows-like-snakemake",
    "title": "5  WDL for you and me",
    "section": "5.2 How is it different from workflows like Snakemake?",
    "text": "5.2 How is it different from workflows like Snakemake?\nWDL leverages reproducible software environments called containers rather than relying on pre-installed software. File paths to installed software may not be consistent, and creating a combination of software packages that work on another system can be difficult.\nBy bundling software and their dependencies into a container, the same exact software environment can be reproduced across multiple machines and operating systems.\nTheoretically, given a WDL file, you should be able to reproduce and scale an analysis pipeline.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>WDL for you and me</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A DaSL Community Handbook",
    "section": "",
    "text": "1 Introduction\nHi, my name is Ted Laderas, and I consider myself an educator/community builder first, and a bioinformatician second. I’ve worked with HPC and cloud platforms for doing bioinformatics work, and I want to help you get better at these skills. I believe that if you want to learn to get better at data science, you deserve an opportunity to expand and grow.\nI started my journey a long time ago when most of the skills at the command line I learned off the street. But things don’t have to be that way.\nI hope to encourage and grow the developer community at Fred Hutch into getting better at what they need to do. I know that every one of us is unique, but our commonality is that we want to learn new things. And I know that learning together is one of the most powerful ways to grow.\nThis means carving out a space where it’s ok to ask questions of things that really aren’t obvious but that we pretend that are. I think that a lot of us think we should know something,",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#making-mistakes-is-ok",
    "href": "index.html#making-mistakes-is-ok",
    "title": "A DaSL Community Handbook",
    "section": "2.1 Making Mistakes is OK",
    "text": "2.1 Making Mistakes is OK\nI’ve learned that to get better, you have to be willing to make a lot of mistakes. This is especially true when you are learning to speak a new language.\nAgain, it’s OK not to know how to do things. We have to remember that we are all beginners at something and experts at other things. Let’s be respectful of that.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "hpc_for_scientists.html",
    "href": "hpc_for_scientists.html",
    "title": "4  HPC for Scientists",
    "section": "",
    "text": "4.1 The main players\nBefore we go any further, let’s talk about the key players in a High Performance Computing (hpc) system:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>HPC for Scientists</span>"
    ]
  },
  {
    "objectID": "hpc_for_scientists.html#the-main-players",
    "href": "hpc_for_scientists.html#the-main-players",
    "title": "4  HPC for Scientists",
    "section": "",
    "text": "Head Node: this node controls access to the other nodes in the cluster, which are called worker nodes\nWorker Node: a member of a HPC cluster\nAllocation: a set of nodes that meet a particular set of requirements that is requested by the user. For example, one kind of allocation might be to have 3 nodes with multiple GPUs for machine learning tasks.\nShared Filesystem - a distributed filesystem that can be seen by all of the nodes in a cluster. Examples include Lustre.\nScratch Filesystem - the “local” storage of a worker node",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>HPC for Scientists</span>"
    ]
  },
  {
    "objectID": "hpc_for_scientists.html#relationships-between-the-main-players",
    "href": "hpc_for_scientists.html#relationships-between-the-main-players",
    "title": "4  HPC for Scientists",
    "section": "4.2 Relationships between the main players",
    "text": "4.2 Relationships between the main players\nClusters are ruled by a head node, which as you night guess, tells the other nodes (called worker nodes what to do. It does this by assigning jobs to groups of nodes. These groups of nodes are also called allocations.\nAllocations are requested by users when they submit a job to the head node. Jobs consist of a script or command with software, data to process, and an allocation. Based on the usage and what nodes are available, the head node places the jobs in a queue. If an allocation is available, then the job gets moved from the queue to the allocation.\nJobs can also have subjobs. This is useful because we often want a multiple nodes to process separate files in a job, where a single node can process a single file\nThe Shared Filesystem is readable and writeable by all nodes in the HPC cluster, which means that we can store our data and results there. The head node is responsible for all transfers into and out of the shared filesystem. In very special cases, we need to use local storage (also called scratch storage) to run our job.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>HPC for Scientists</span>"
    ]
  },
  {
    "objectID": "hpc_for_scientists.html#an-hpc-workflow",
    "href": "hpc_for_scientists.html#an-hpc-workflow",
    "title": "4  HPC for Scientists",
    "section": "4.3 An HPC workflow",
    "text": "4.3 An HPC workflow\nHigh Performance Computing clusters are shared resources. We have to share computing resources with others.\nThe main mechanism for sharing compute time equitably among users is the queue. THe queue contains a list of jobs and the compute requested for the job, whether it be a 96 core machine, or a 10 8 core machines. Based on the usage of others, the head node will order your submitted job in relation to other submitted jobs.\nThere is also a calculation based on the scarcity of a compute resource - if you request allocations with a large number of cores that are in demand, it will take longer to get your job done.\nPart of getting your jobs done effectively on HPC is breaking up your jobs into smaller subjobs that can be run on the most available nodes.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>HPC for Scientists</span>"
    ]
  },
  {
    "objectID": "hpc_for_scientists.html#break-it-up",
    "href": "hpc_for_scientists.html#break-it-up",
    "title": "4  HPC for Scientists",
    "section": "4.4 Break it Up",
    "text": "4.4 Break it Up\nA lot of times, when we do genomics work, we want to process many files at a time. For example, we might want to",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>HPC for Scientists</span>"
    ]
  },
  {
    "objectID": "learning_paths.html#a-learning-path-to-hpc",
    "href": "learning_paths.html#a-learning-path-to-hpc",
    "title": "2  Learning Paths",
    "section": "2.2 A Learning Path to HPC",
    "text": "2.2 A Learning Path to HPC\nIt took me a long while to pick up these skills. I spent a long time reading documentation. Part of what drove me was the desire to teach others.\nI think that there are multiple skills and levels that we need to build to be effective and reproducible with HPC. Here’s the list:\n\nUp your bash skills\nBash Scripting\nLearning about HPC concepts and allocations\nUp your knowledge about filesystems and naming\nWDL and Docker for reproducibility\nScattering your data and bringing it back together\n\nPart of the purpose of listing these skills is to help you find a starting point in your journey.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learning Paths</span>"
    ]
  },
  {
    "objectID": "learning_paths.html#up-your-knowledge-about-filesystems",
    "href": "learning_paths.html#up-your-knowledge-about-filesystems",
    "title": "2  Learning Paths",
    "section": "2.5 Up Your Knowledge about Filesystems",
    "text": "2.5 Up Your Knowledge about Filesystems\nWe want to learn recipes for listing files in a directory so we can process them. We also want to learn how to traverse multiple directories and store our results in separate folders.\nWe can extend our shell scripting skills to utilize a file manifest - a list of files we want to process - and process them with our homemade pipelines.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learning Paths</span>"
    ]
  },
  {
    "objectID": "learning_paths.html#wdldocker",
    "href": "learning_paths.html#wdldocker",
    "title": "2  Learning Paths",
    "section": "2.7 WDL/Docker",
    "text": "2.7 WDL/Docker\nYou’re probably familiar with the issue of reproducing an analysis - a large part is reproducing the software environment that was used in the original analysis. The chain of software dependencies can affect whether or not a script can run in a different software environment.\nEnter containers, which let you freeze version of your software packages and environment so your scripts will run exactly as intended on any machine. This males your tools portable and is another step in making your work reproducible.\nWhat you can do now: run your WDLized script on nearly any machine, including HPC clusters and cloud systems.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learning Paths</span>"
    ]
  },
  {
    "objectID": "learning_paths.html#scattering-your-jobs",
    "href": "learning_paths.html#scattering-your-jobs",
    "title": "2  Learning Paths",
    "section": "2.8 Scattering your jobs",
    "text": "2.8 Scattering your jobs\nA lot of what we do as bioinformaticists is process large folders of files, such as VCF files. Splitting up the work so that it runs on multiple machines is straightforward: we usually want to have a single node process one file at a time in our pipeline.\nWDL gives us a mechanism to split up the data files: it’s called scatter will divide up the data into independent subjobs that can be assigned to run. The nice thing about scatter is that it adapts to the type of computer architecture that you have: whether it be a single node. Multiple nodes in a HPC or multiple workers in the cloud, it will run appropriately.\nThis does require that the system we’re working on is configured correctly.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Learning Paths</span>"
    ]
  }
]