---
title: "Learning Paths"
---

## The Purpose of This Chapter

The purpose of this chapter is to list skills that are necessary to be successful in reproducibly running workflows on a High Performance Computing Cluster.

This chapter exists to help you find out what you know and what you might want to know. Each skill highlights what you can accomplish once you've mastered it. 

## A Learning Path to HPC

It took me a long while to pick up these skills. I spent a long time reading documentation. Part of what drove me was the desire to teach others.   

I think that there are multiple skills and levels that we need to build to be effective and reproducible with HPC. Here's the list:

- Up your bash skills
- Bash Scripting
- Learning about HPC concepts and allocations
- Up your knowledge about filesystems and naming
- WDL and Docker for reproducibility
- Scattering your data and bringing it back together

Part of the purpose of listing these skills is to help you find a starting point in your journey. 

## Up Your Shell Skills

Let's face it - almost none of us have formal training in using the Unix Shell. And yet there are some of us who seem like wizards at the shell - their commands and flle names seem to complete if by magic, and they can run jobs and sign-off without interrupting their jobs. 

Let's throw back the curtain once and for all - this is all knowledge you can pick up and excel at. 
 
**What you can do now:** Run background processes while being signed out, navigate relative paths, use keyboard shortcuts such as tab completion and history. 

## Bash Scripting to Automate the Boring Things

Now we have some skill navigating the shell and our file system. We can start thinking about automating our work. We can take a list of files and process them with the same commands, we can schedule processes to run at night, and we can start adapting or R scripts to run on the command line. 

It all starts with the parametrization of our scripts and learning how to pipe things: taking the output of one command and using it as the input for another command.  Once we learn that, we can start building larger pipelines to process files. 

**What you can do now:** Build simple pipelines gluing multiple software together. Process a file with a script using our pipeline. Install software to a system without root access.

## Up Your Knowledge about Filesystems

We want to learn recipes for listing files in a directory so we can process them. We also want to learn how to traverse multiple directories and store our results in separate folders.  

We can extend our shell scripting skills to utilize a file manifest - a list of files we want to process - and process them with our homemade pipelines. 

## Learn HPC Concepts

We've done everything so far on a single computer - now we can graduate to the HPC cluster. We need to learn a bit about the architecture of the cluster and how to use it. 

We need to learn about the different types of nodes, how we can use them, and how to access software and data within the shared filesystem. 

**What you can do now:** visualize the flow of requesting an allocation, query the head node to see how busy the cluster is, understand our place in a job queue and how we can speed up processing jobs.

## WDL/Docker

You're probably familiar with the issue of reproducing an analysis - a large part is reproducing the software environment that was used in the original analysis. The chain of software dependencies can affect whether or not a script can run in a different software environment. 

Enter containers, which let you freeze version of your software packages and environment so your scripts will run exactly as intended on any machine. This males your tools portable and is another step in making your work reproducible. 

**What you can do now**: run your WDLized script on nearly any machine, including HPC clusters and cloud systems. 

## Scattering your jobs

A lot of what we do as bioinformaticists is process large folders of files, such as VCF files. Splitting up the work so that it runs on multiple machines is straightforward: we usually want to have a single node process one file at a time in our pipeline. 

WDL gives us a mechanism to split up the data files: it's called scatter. 
